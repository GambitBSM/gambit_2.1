#  GUM: GAMBIT Universal Model Machine
#  ************************************
#  \file
#
#  Main GUM script
#
#  *************************************
#
#  \author Sanjay Bloor
#          (sanjay.bloor12@imperial.ac.uk)
#  \date 2018, 2019, 2020...
#
#  \author Tomas Gonzalo
#          (tomas.gonzalo@monash.edu)
#  \date 2019, 2020
#
#  **************************************

from __future__ import print_function

import sys
import platform
import os

import argparse
import numpy as np

from src import *

from collections import defaultdict
from distutils.dir_util import copy_tree

try:
    # Python 2
    xrange
except NameError:
    # Python 3, xrange is now named range
    xrange = range

print()
print(banner())
print("-- Running GUM with python version", platform.python_version(), "--")
print()

parser = argparse.ArgumentParser(description="From Lagrangian to scans: GUM "
                                             "(GAMBIT Universal Models)")

# Optional command-line arguments
parser.add_argument('-f', '--file', type=str,
                      help="Specify input .GUM file.")
parser.add_argument("-d", "--dryrun", action='store_true',
                    help="GUM will perform a dry run, not saving any output.")
parser.add_argument("-r", "--reset", type=str,
                    help=("GUM will reset GAMBIT back to a previous version, "
                         "given an input .mug file generated by GUM."))
# Retrieve command-line arguments
args = parser.parse_args()

# Don't let the user create and reset at the same time
if args.file and args.reset:
    raise GumError(("\n\n\tYou may use only one of the "
                    "--file or --reset flags at any one time!"))

if args.reset and args.dryrun:
    raise GumError(("\n\n\tYou have requested a dryrun and a reset together:"
                    " this is not supported."))

# Input .gum file.
if args.file:

    if args.dryrun:
        print("**********************************************************")
        print("GUM called with a dry run -- will not be writing to files!")
        print("**********************************************************")

    try:
        # Parse the .gum file
        inputs = check_gum_file(args.file)
        gum, output_opts = fill_gum_object(inputs)

        # SARAH removes hyphens and underscores for .f90 files...
        clean_model_name = gum.name.replace('-','').replace('_','')

        # Create the output directory for all generated files for this model
        output_dir = os.path.join(os.getcwd(),"Outputs", gum.name)
        mkdir_if_absent(output_dir)

        # Check for a DM candidate
        if gum.dm_pdg:
            darkbit = True
        else:
            darkbit = False
            print(("No DM candidate requested -- no DarkBit routines will "
                   "be written.\n"))

        # Check whether we need to generate DecayBit output or not
        # N.B. if we don't something is quiiiiite likely to go wrong
        if output_opts.ch or output_opts.spheno:
            decaybit = True
        else:
            decaybit = False

        # Check whether we need to generate new ColliderBit code or not
        # Also check whether the correct YAML entries are filled
        if output_opts.pythia:
            colliderbit = True
            if not 'pythia' in output_opts.options:
                raise GumError(("\n\nPythia output requested but no "
                                "collider_processes specified.\n"
                                "Please amend your .gum file!"))
            collider_processes = output_opts.options['pythia'].get('collider_processes')
            if collider_processes == None:
                raise GumError(("\n\nPythia output requested but no "
                                "collider_processes specified.\n"
                                "Please amend your .gum file!"))
        else:
            colliderbit = False

        # Check to see if any of the proposed new files will
        # clash with any existing files.
        # for either the model name or clean model name
        check_for_existing_entries(gum.name, darkbit, colliderbit, output_opts)
        check_for_existing_entries(clean_model_name, darkbit, colliderbit, output_opts)

        """
        FEYNRULES/SARAH
        """
        if gum.math == 'feynrules':

            # Let GUM do a quick and dirty parse of the file first, before we
            # fire up a Mathematica kernel
            parse_feynrules_model_file(gum.mathname, gum.base_model,
                                       output_opts)

            from lib.libfr import *

            # Create the output directory for all generated FeynRules files
            mkdir_if_absent(output_dir + "/FeynRules")

            options = FROptions("feynrules", gum.mathname, gum.base_model,
                                gum.restriction, gum.LTot)
            partlist = FRVectorOfParticles()
            paramlist = FRVectorOfParameters()
            outputs = FROutputs()
            backends = FRBackends()
            backends.extend(x for x in output_opts.bes())
            mixings = {}
            err = FRError()

            # Hit it
            all_feynrules(options, partlist, paramlist, outputs, backends, err)

            if err.is_error():
                raise GumError( err.what() )

            # Get a list of all non-SM particles
            fr_bsm = [x for x in partlist if x.SM() is False]

            # Make all BSM particles work with GUM's native Particle class.
            # Find out if the spectrum needs a dependency on StandardModel_Higgs
            bsm_particle_list, add_higgs = fr_part_to_gum_part(fr_bsm)

        elif gum.math == 'sarah':

            # Let GUM do a quick and dirty parse of the file first, before we
            # fire up a Mathematica kernel
            parse_sarah_model_file(gum.mathname, output_opts)

            from lib.libsarah import *

            # Create the output directory for all generated SARAH files
            mkdir_if_absent(output_dir + "/SARAH")

            options = SARAHOptions("sarah", gum.mathname)
            options.setOptions(**output_opts.options)
            partlist = SARAHVectorOfParticles()
            paramlist = SARAHVectorOfParameters()
            flags = SARAHMapStrBool()
            outputs = SARAHOutputs()
            bcs = SARAHMapStrStr()
            mixings = SARAHMapStrStr()
            sphenodeps = SARAHVectorOfParameters()
            backends = SARAHBackends()
            backends.extend(x for x in output_opts.bes())
            err = SARAHError()

            # Hit it
            all_sarah(options, partlist, paramlist, outputs, backends, flags,
                      mixings, bcs, sphenodeps, err)

            if err.is_error():
                raise GumError( err.what() )

            # Get a list of all non-SM particles
            sarah_bsm = [x for x in partlist if x.SM() is False]

            # Make all BSM particles work with GUM's native Particle class.
            bsm_particle_list, add_higgs = sarah_part_to_gum_part(sarah_bsm)

        print("Finished extracting parameters from " + gum.math + ".")

        # Check to see if the DM particle exists in the model file.
        # First, initialise DM particle from particle list
        if darkbit:
            dm_set = False
            for i in xrange(len(partlist)):
                part = partlist[i]
                # If specified, initialise the DM candidate
                if part.pdg() == gum.dm_pdg:
                    dm = Particle(part.name(), part.antiname(),
                                  part.spinX2(), part.pdg(),
                                  part.mass(), part.chargeX3(), part.color())
                    dm_set = True

            # If we haven't found the DM candidate in the particle list,
            # throw an error
            if not dm_set:
                raise GumError(("\n\nThe WIMP candidate specified has not been "
                                "found in the {0} file. Please check your .gum "
                                "file and {0} file are consistent.\n"
                                "PDG code given = {1}"
                                ).format(gum.math, gum.dm_pdg))

            # If the user has requested MicrOMEGAs output and the DM candidate
            # does not begin with a tilde, then complain (and its anti-p.)
            if output_opts.mo:
                if not dm.name.startswith('~'):
                    raise GumError(("You have requested MicrOMEGAs output, but "
                                    " your DM candidate has to begin with a '~'"
                                    " for MicrOMEGAs to recognise it as an odd "
                                    "particle.\n Please change the DM name in "
                                    "your {} file!"
                                    ).format(gum.math))
                if not dm.antiname.startswith('~'):
                    raise GumError(("You have requested MicrOMEGAs output, but "
                                    " your DM candidate's antiparticle also has"
                                    " to begin with a '~' for MicrOMEGAs to "
                                    "recognise it as an odd particle.\n "
                                    "Note that the default behaviour will be "
                                    "to just add a tilde at the end, and strip "
                                    "any leading tilde, so add a line like:\n"
                                    "\t    AntiParticleName -> \"~Chi\"\n "
                                    "to your {} file!"
                                    ).format(gum.math))


        """
        UFO2MDL

        If we have both UFO and MDL files, run the files through ufo2mdl to
        compare outputs. This will, e.g., add 4-fermion interactions to CalcHEP
        where FeynRules fails to do so.
        """
        # if output_opts.ch and output_opts.ufo:
        #     print("Calling UFO2MDL.")
        #     blockPrint()  # Stop the UFO2MDL output - there's lots of it
        #     ch_location = compare_mg_and_ch(outputs.get_mg(), outputs.get_ch())
        #     enablePrint() # Bring it back.
        #     if ch_location != outputs.get_ch():
        #         print("Updating location of CalcHEP files in GUM...")
        #         outputs.set_ch(ch_location)

        # Move MadGraph files from SARAH/FeynRules to contrib/MadGraph/models
        # TODO make this go to the Outputs dir instead, then ln -s from there
        # to MadGraph later.
        if output_opts.ufo:
            copy_madgraph_files(outputs.get_mg(), gum.name)

        # Clean up calchep files from SARAH/FeynRules, then copy them to the output dir
        if output_opts.ch or output_opts.mo:
            clean_calchep_model_files(outputs.get_ch(), gum.name, output_dir)

        """
        MADGRAPH -- Pythia inside here (and MadDM in the future)
        """
        if output_opts.ufo:

            # Pythia
            if output_opts.pythia:

                collider_processes = output_opts.options['pythia'].get('collider_processes')
                multiparticles = output_opts.options['pythia'].get('multiparticles')
                if collider_processes is not None:
                    # Link MadGraph files output by SARAH/FeynRules to
                    # MadGraph's contrib/MadGraph/models
                    # still TODO

                    # Create the output directory for all generated MadGraph
                    # files
                    mg5_output_dir = output_dir + "/MadGraph5_aMC"
                    mkdir_if_absent(mg5_output_dir)

                    # Clear and remake the copy of Pythia that will receive the
                    #  matrix element code produced by MadGraph
                    pristine_pythia_dir = os.path.join(os.getcwd(),
                                                       'contrib','Pythia')
                    new_pythia_dir = mg5_output_dir + "/Pythia_patched"
                    remove_tree_quietly(new_pythia_dir)

                    copy_tree(pristine_pythia_dir, new_pythia_dir)

                    # Create the MadGraph script
                    # TODO determine all possible BSM processes automatically
                    # when collider_processes is missing from the .gum file
                    mg5_dir = os.path.join(os.getcwd(),'contrib','MadGraph')
                    make_madgraph_script(mg5_dir, mg5_output_dir, gum.name,
                                         collider_processes,
                                         multiparticles)

                    # Add MadGraph to path and import the python interface
                    sys.path.append(mg5_dir)
                    import madgraph.interface.master_interface as mi

                    # Run MadGraph
                    call_madgraph(mg5_dir, mg5_output_dir, mi)

                else:
                    raise GumError(("Pythia output requested but no "
                                    "collider_processes specified.\n"
                                    "Please amend your .gum file!"))

        print("")
        print("Finished running external codes...")
        print("Now attempting to write proposed GAMBIT code.")

        # Create defaultdict for reset file.
        reset_contents = defaultdict(lambda: defaultdict(list))

        # Dictionaries with capability and model definitions to add to the list
        capability_definitions = {}
        model_definitions = {}

        """
        REGISTRATION OF NEW PARTICLES
        """

        # Need to know how GAMBIT refers to each particle. Scrape information
        # from particle_database.yaml.
        gambit_pdgs, decaybit_dict = get_gambit_particle_pdg_dict()

        # Check every new particle is in the database
        missing_parts = check_all_particles_present(partlist, gambit_pdgs)

        if len(missing_parts) != 0:
           # Add new particles to the database, refresh the dicts
           gambit_pdgs, decaybit_dict, particleDB = \
               add_new_particleDB_entry(missing_parts, gum.dm_pdg,
                                        gambit_pdgs, decaybit_dict,
                                        reset_contents, gum.name,
                                        gum.dm_decays)

        # Grab the antiparticles
        antiparticle_dict = get_antiparticles(partlist)

        # Return a list of the PDG codes of all Higgses involved
        higgses, neutral_higgses, charged_higgses = \
            get_higgses(bsm_particle_list)

        """
        PARAMETERS
        """

        # Initialise all of the model parameters, for writing spectra,
        # model files - remove all SM stuff from parameter list.
        if gum.math == "feynrules":
            parameters = fr_params(paramlist, add_higgs)
        else:
            parameters = sarah_params(paramlist, mixings, add_higgs,
                                      gambit_pdgs, partlist, bcs)

        # Get the parameters sorted by block
        blockparams = sort_params_by_block(parameters, mixings)
        # Dict of which parameters are real.
        reality_dict = parameter_reality_dict(parameters)

        """
        MODELS
        """

        print("Adding new model {0} to GAMBIT.".format(gum.name))

        # For FeynRules, use masses of particles as input parameters
        if gum.math == 'feynrules':
            # Add all of the masses of BSM particles as pole masses.
            # Also removes all duplicate entries that can arise
            # for e.g. multiplets with same tree-level masses
            parameters = add_masses_to_params(parameters, bsm_particle_list,
                                              gambit_pdgs, add_higgs)

        # Get the model parameters out of the parameter list
        model_parameters = get_model_parameters(parameters, add_higgs)

        model_header = add_to_model_hierarchy(gum.spec, gum.name,
                                              model_parameters, model_definitions,
                                              capability_definitions)

        # Get the spectrum parameters out of the parameter list
        spectrum_parameters = get_spectrum_parameters(parameters, blockparams,
                                                      bsm_particle_list,
                                                      partlist, gambit_pdgs,
                                                      output_opts.spheno)

        subspec_wrapper = write_subspectrum_wrapper(gum.name,
                                                    spectrum_parameters)
        spec_contents = write_spectrumcontents(gum.name, spectrum_parameters)
        reg_spec, reg_spec_num = add_to_registered_spectra(gum.name)

        """
        SPHENO
        """

        # Forward declare some variables
        decays = {}
        spheno_decays = {}

        if output_opts.spheno:

            # Move SPheno files from SARAH to Outputs.
            copy_spheno_files_output(clean_model_name, output_dir, SPHENO_DIR,
                              outputs.get_sph())

            # Pristine and patched SPhenos
            pristine_spheno_dir = output_dir + "/SPheno"
            patched_spheno_dir = output_dir + "/SPheno_patched"

            # Patch the files
            print("Patching SPheno...")

            patch_spheno(clean_model_name, patched_spheno_dir, flags,
                         bsm_particle_list)

            fullpath = "Backends/patches/sarah-spheno/{0}/{1}".format(
                                                               SPHENO_VERSION,
                                                               gum.name)
            fullfile = "patch_sarah-spheno_"+SPHENO_VERSION+"_"+gum.name
            # Create a diff vs. the out of the [SARAH]box
            write_backend_patch(output_dir, pristine_spheno_dir,
                                patched_spheno_dir,
                                "sarah-spheno",
                                SPHENO_VERSION,
                                fullpath = fullpath,
                                fullfile = fullfile)


            # Get the defs of any SPheno dependencies
            deps = spheno_dependencies(sphenodeps)

            # Write frontends for SPheno. That's a lotta scrapin'.
            print("Writing SPheno frontends.")
            spheno_src, spheno_header, backend_types, btnum = \
                write_spheno_frontends(clean_model_name, parameters,
                                       bsm_particle_list, flags,
                                       patched_spheno_dir, output_dir,
                                       blockparams, gambit_pdgs, mixings,
                                       reality_dict, deps, bcs,
                                       charged_higgses, neutral_higgses,
                                       gum.name, capability_definitions)

            # cmake entry
            spheno_cmake = write_spheno_cmake_entry(gum.name, SPHENO_VERSION,
                                                    clean_model_name)

            print("Writing SPheno decay table.")
            spheno_decay_tables, spheno_decays = \
                make_spheno_decay_tables(patched_spheno_dir, clean_model_name)

            # DecayBit entry
            print("Writing SPheno module functions for DecayBit.")
            spheno_decay_src, spheno_decay_header = \
                write_spheno_decay_entry(gum.name, clean_model_name)

            # HiggsCouplingsTable
            hct_src, hct_head, hb_pattern = new_hct_switch(gum.name, gum.spec,
                                                           neutral_higgses,
                                                           gambit_pdgs,
                                                           len(higgses))

            # Save this dictionary as the global 'decays' for other backends
            decays = spheno_decays

        """
        CALCHEP
        """

        # Obtain all model interactions from CalcHEP model files.
        # Also grab the PDG codes of each particle and the names of their
        # masses -- we'll want to use this for writing the micromegas frontend
        calchep_pdg_codes = {}
        if output_opts.ch or output_opts.mo:

            (interactions, calchep_pdg_codes, calchep_masses,
            calchep_widths, aux_particles) = get_vertices(outputs.get_ch())

        if output_opts.ch:

            # Save a dictionary of all processes that GAMBIT may write matrix
            # element code for in CalcHEP
            ch_processes = defaultdict(lambda: defaultdict(list))

            # Obtain all 3-point vertices that are BSM.
            three_pi_bsm = [i.particles for i in interactions
                            if i.num_particles() == 3 and i.is_sm() == False]

            three_body_decays = decay_sorter(three_pi_bsm, aux_particles,
                                             antiparticle_dict)

            # If SPheno hasn't been run then save the tree-level decays from CH
            # to the global 'decays' variable
            if not bool(decays):
                decays = ch_decays_to_dict(three_body_decays)


            print("Writing CalcHEP module functions for DecayBit")
            # CalcHEP specific decays
            decaybit_src_ch = ""

            """
            DECAYBIT
            """
            # TODO: if we have SPheno output, do we /really/ want
            # to provide CalcHEP DecayBit entries too?

            # Pass all interactions by first PDG code needed.
            for i in xrange(len(three_body_decays)):
                decaybit_src_ch += write_decaytable_entry_calchep(
                                                          three_body_decays[i],
                                                          gum.name,
                                                          calchep_pdg_codes,
                                                          gambit_pdgs,
                                                          decaybit_dict,
                                                          ch_processes)

            decay_roll, new_decays = \
                write_decaybit_rollcall_entry_calchep(gum.name, gum.spec,
                                                      three_body_decays,
                                                      decaybit_dict,
                                                      gambit_pdgs,
                                                      capability_definitions)

            all_decays_src, all_decays_header = \
                amend_all_decays_calchep(gum.name, gum.spec, new_decays)

        """
        COLLIDERBIT
        """
        if colliderbit:

            print("Writing new module functions for ColliderBit")

            # Create the output directory for all generated ColliderBit sources
            cb_output_dir = output_dir + "/ColliderBit"
            mkdir_if_absent(cb_output_dir)

            # Write ColliderBit headers and sources for the new model
            new_colliderbit_model(cb_output_dir, gum.name)
            
        """
        DARKBIT
        """

        if darkbit:

            # Set the following variables to None for now - if they're not None,
            # they need to be computed and passed to the writing routines.
            pc = None            # Do we write the Process Catalogue?
            sv = None            # Do we figure out cross-sections?
            products = None      # What are the products of DM ann./decay?
            propagators = None   # Any propagators involved?

            # If we have decaying DM, we can write the PC with any DecayTable.
            if gum.dm_decays == True:
                pc = True

            print("Writing new module functions for DarkBit")

            # Conditional on CalcHEP files being made.
            if output_opts.ch:

                # Definitely write process catalogue output
                pc = True

                # If DM doesn't decay (i.e. it annihilates), then
                # we want to use the sv (= 'sigma*v') routines.
                if gum.dm_decays == False:
                    sv = True

                    # Obtain all three-field vertices
                    three_f = [i.particles for i in interactions
                               if i.num_particles() == 3]
                    # And all 4-point BSM vertices
                    four_f = [i.particles for i in interactions
                             if i.num_particles() == 4 and i.is_sm() == False]

                    # Get all annihilation products and propagators involved in
                    # DM + DM -> X + Y at tree-level
                    products, propagators = \
                        sort_annihilations(dm, three_f, four_f, aux_particles,
                                           antiparticle_dict)

                # Otherwise - products are the decays of the DM particle.
                else:
                    products = decays[gum.dm_pdg]

            # If micrOMEGAs requested
            if output_opts.mo:

                print("Writing micrOMEGAs interface for DarkBit.")

                mo_src = write_micromegas_src(gum.name, gum.spec, gum.math,
                                              parameters, bsm_particle_list,
                                              gambit_pdgs, calchep_masses,
                                              calchep_widths)
                mo_head = write_micromegas_header(gum.name, gum.math,
                                                  parameters, capability_definitions)

            darkbit_src = write_darkbit_src(dm, pc, sv, products,
                                            propagators, gum.dm_decays,
                                            gambit_pdgs, gum.name,
                                            calchep_pdg_codes,
                                            bsm_particle_list, higgses,
                                            ch_processes)

            pc_cap, dmid_cap, dmconj_cap = write_darkbit_rollcall(gum.name, pc,
                                                                  gum.dm_decays)
                                                                  
            wimp_prop_h, wimp_prop_c = write_wimp_props(gum.name)

        """
        CALCHEP SRC
        Must be done after DecayBit & DarkBit as we want the matrix elements.
        """

        if output_opts.ch:
            # Entries for header file
            ch_src_sl, ch_src_pl, ch_head = add_calchep_switch(gum.name,
                                                               gum.spec,
                                                               ch_processes)


        """
        SPECBIT
        """

        print("Writing basic container SpecBit interface...")
        spectrum_src, higgsdefined = write_spectrum(gum.name, parameters,
                                                    gum.spec, add_higgs,
                                                    output_opts.spheno,
                                                    gambit_pdgs,
                                                    neutral_higgses,
                                                    charged_higgses,
                                                    blockparams,
                                                    bsm_particle_list,
                                                    spheno_decays, partlist)
        # If the Higgs has now been defined by some other parameters,
        # don't need to have mH as a model param.
        if higgsdefined: add_higgs = False

        spectrum_header = write_spectrum_header(gum.name, add_higgs,
                                                output_opts.spheno, higgses,
                                                capability_definitions)
        spec_rollcall = write_specbit_rollcall(gum.name)

        """
        VEVACIOUS
        """

        if output_opts.vev:
            vev_src = write_vevacious_src(gum.name, outputs.get_vev(), gum.spec,
                                          blockparams)

        """
        PYTHIA
        """

        # Have we created a new Pythia backend?
        if output_opts.pythia:
            print(("Patching Pythia to inject new matrix elements into its "
                   "Process Container and shared library."))
            pythia_groups = output_opts.options['pythia'].get('pythia_groups')
            fix_pythia_lib(gum.name, new_pythia_dir, pythia_groups,
                           bsm_particle_list, decays)
            print("Creating a diff vs original version of Pythia.")
            write_backend_patch(output_dir, pristine_pythia_dir, new_pythia_dir,
                                "pythia_"+gum.name.lower(),
                                "8."+base_pythia_version)
            print("Writing an additional patch for the new version of Pythia.")
            patch_pythia_patch(parameters, gum.name, reset_contents)
            print("Creating a cmake entry for Pythia"+gum.name+".")
            pythia_cmake = write_pythia_cmake_entry(gum.name, output_dir)
            print(("Setting the default version of Pythia_"+gum.name+" for "
                   "BOSSed classes to 8."+base_pythia_version))
            write_pythia_capability_defs(gum.name, capability_definitions)

            # Adding in a UserHook
            print("Writing a Pythia UserHooks class for ColliderBit")
            set_userhook = write_set_userhook(gum.name,base_pythia_version)
            apply_userhook = write_apply_userhook(gum.name)

        # Stop now if we're just doing a dry run
        if args.dryrun:
            print("")
            print("Dry run finished.")
            print("")
            exit()

        #################################################################
        ################### DRY RUN STOPS HERE ##########################
        # All file writing routines HERE, once everything has gone okay.#
        #################################################################

        print("")
        print("Now putting the new code into GAMBIT.")

        # ParticleDB
        if len(missing_parts) :
            write_particleDB(particleDB)

        # Models
        m = "Models"
        write_file("models/" + gum.name + ".hpp", m, model_header,
                   reset_contents)
        write_file("SpectrumContents/" + gum.name + ".cpp", m, spec_contents,
                   reset_contents)
        write_file("SimpleSpectra/" + gum.name + "SimpleSpec" + ".hpp", m,
                    subspec_wrapper, reset_contents)
        amend_file("SpectrumContents/RegisteredSpectra.hpp", m, reg_spec,
                    reg_spec_num, reset_contents)


        # SpecBit
        m = "SpecBit"
        write_file("SpecBit_" + gum.name + ".cpp", m, spectrum_src,
                   reset_contents)
        write_file("SpecBit_" + gum.name + "_rollcall.hpp", m, spectrum_header,
                    reset_contents)
        num = find_string("SpecBit_rollcall.hpp", m,
                          "SpecBit_tests_rollcall.hpp")[1]
        amend_file("SpecBit_rollcall.hpp", m, spec_rollcall, num,
                   reset_contents)

        # DecayBit
        if decaybit:
            m = "DecayBit"
            # Append to the end of DecayBit -- just before all_decays
            num = find_string("DecayBit.cpp", m, "void all_decays")[1]
            if output_opts.ch:
                amend_file("DecayBit.cpp", m, decaybit_src_ch, num-3,
                           reset_contents)
                for i in xrange(len(decay_roll)):
                    if find_capability(decay_roll[i][0], m)[0]:
                        amend_rollcall(decay_roll[i][0], m, decay_roll[i][1],
                                       reset_contents)
                    else:
                        num = find_string("DecayBit_rollcall.hpp", m,
                                          "#define CAPABILITY decay_rates")[1]
                        amend_file("DecayBit_rollcall.hpp", m, decay_roll[i][1],
                                   num-2, reset_contents)
                if len(new_decays) > 0:
                    num = find_string("DecayBit_rollcall.hpp", m,
                                      "MODEL_CONDITIONAL_DEPENDENCY"\
                                      "(MSSM_spectrum")[1]
                    amend_file("DecayBit_rollcall.hpp", m, all_decays_header,
                               num-1, reset_contents)
                    num = find_string("DecayBit.cpp", m, "decays(\"omega\")")[1]
                    amend_file("DecayBit.cpp", m, all_decays_src, num,
                               reset_contents)
            num = find_string("DecayBit.cpp", m, "void all_decays")[1]
            if output_opts.spheno:
                amend_file("DecayBit.cpp", m, spheno_decay_src, num-3,
                            reset_contents)
                num = find_string("DecayBit_rollcall.hpp", m,
                                  "#define CAPABILITY decay_rates")[1]
                amend_file("DecayBit_rollcall.hpp", m, spheno_decay_header,
                           num+2, reset_contents)

        # DarkBit
        if darkbit:
            m = "DarkBit"
            amend_rollcall("DarkMatter_ID", m, dmid_cap, reset_contents)
            amend_rollcall("DarkMatterConj_ID", m, dmconj_cap,
                           reset_contents)
            # Add to the wimp properties
            num = find_string("DarkBit_rollcall.hpp", m,"MODEL_CONDITIONAL_DEPENDENCY(DMEFT_spectrum, Spectrum, DMEFT)")[1]
            amend_file("DarkBit_rollcall.hpp", m, wimp_prop_h,num, reset_contents)
            
            num = find_string("DarkBit.cpp", m,"if(ModelInUse(\"DMEFT\"))")[1]
            amend_file("DarkBit.cpp", m, wimp_prop_c,num-1, reset_contents)
            
            write_file(gum.name + ".cpp", m, darkbit_src, reset_contents)
            if pc:
                amend_rollcall("TH_ProcessCatalog", m, pc_cap, reset_contents)
            # If DM isn't decaying, we can use the built-in
            # relic density routines. If not: no, they won't work.
            if pc and not gum.dm_decays:
                add_new_model_to_function("DarkBit_rollcall.hpp", "DarkBit",
                                          "RD_spectrum",
                                          "RD_spectrum_from_ProcessCatalog",
                                          gum.name, reset_contents,
                                          pattern="ALLOW_MODELS")
                add_new_model_to_function("DarkBit_rollcall.hpp", "DarkBit",
                                          "RD_eff_annrate",
                                          "RD_eff_annrate_from_ProcessCatalog",
                                          gum.name, reset_contents,
                                          pattern="ALLOW_MODELS")

        # ColliderBit
        m = "ColliderBit"
        if output_opts.pythia:
            copy_file("models/"+gum.name+".hpp", m, output_dir, reset_contents,
                      existing = False)
            copy_file("models/"+gum.name+".cpp", m, output_dir, reset_contents,
                      existing = False)

            #Adding in the Set UserHooks Changes.
            num = find_string("colliders/Pythia8/SetHooksClass.hpp", m,
                                  "    class SetHooks")[1]
            amend_file("colliders/Pythia8/SetHooksClass.hpp", m, set_userhook,
                           num+14, reset_contents)

            num = find_string("getPy8Collider.hpp", m,
                                  "          catch (typename Py8Collider<PythiaT,EventT,hepmc_writerT>::InitializationError& e)")[1]
            amend_file("getPy8Collider.hpp", m, apply_userhook,
                           num+10, reset_contents)

            # write all invisible particles in the model to Event header
            num = find_string("heputils/include/HEPUtils/Event.h", "contrib",
                                  "        _cinvisibles.push_back(p);")[1]
            amend_file("heputils/include/HEPUtils/Event.h", "contrib", get_invisibles(gum.invisibles_pdg),
                           num+1, reset_contents)

        # HiggsBounds interface
        if output_opts.spheno:
            if len(higgses) == 1:
                num = find_string("ColliderBit_Higgs.cpp", m,
                        "ModelInUse(\"StandardModel_Higgs\")")[1]
                amend_file("ColliderBit_Higgs.cpp", m, hct_src, num-1,
                           reset_contents)
                num = find_string("ColliderBit_Higgs_rollcall.hpp", m,
                        "MODEL_CONDITIONAL_DEPENDENCY(ScalarSingletDM_Z2")[1]
                amend_file("ColliderBit_Higgs_rollcall.hpp", m, hct_head, num-1,
                           reset_contents)
            else:
                num = find_string("ColliderBit_Higgs.cpp", m,
                        "No valid model for MSSMLikeHiggs_ModelParameters.")[1]
                amend_file("ColliderBit_Higgs.cpp", m, hct_src, num-1,
                           reset_contents)
                num = find_string("ColliderBit_Higgs_rollcall.hpp", m,
                        "MODEL_CONDITIONAL_DEPENDENCY(MSSM_spectrum")[1]
                amend_file("ColliderBit_Higgs_rollcall.hpp", m, hct_head, num-1,
                           reset_contents)
            add_new_model_to_function("ColliderBit_Higgs_rollcall.hpp",
                                      m, "HB_ModelParameters",
                                      hb_pattern, gum.name, reset_contents,
                                      pattern="ALLOW_MODELS")

        # Backends
        m = "Backends"
        rebuild_backends = []

        # CalcHEP
        if output_opts.ch:
            # Move files from output to Gambit dir
            ch_patch_dir = "patches/calchep/3.6.27/Models/" + gum.name + '/'
            ch_dest_dir = "../Backends/" + ch_patch_dir
            ch_src_dir = output_dir + "/" + m + "/" + ch_patch_dir
            remove_tree_quietly(ch_dest_dir)
            mkdir_if_absent(ch_dest_dir, reset_contents)
            for ch_file in os.listdir(ch_src_dir):
                copy_file(ch_file, m, output_dir, reset_contents,
                          existing = False, overwrite_path = ch_patch_dir)
            f = "frontends/CalcHEP_3_6_27.cpp"
            num = find_string(f, m, "setModel(modeltoset, 1)")[1]
            amend_file(f, m, ch_src_sl, num-2, reset_contents)
            num = find_string(f, m, "END_BE_INI_FUNCTION")[1]
            amend_file(f, m, ch_src_pl, num-2, reset_contents)
            f = "frontends/CalcHEP_3_6_27.hpp"
            num = find_string(f, m, "backend_undefs.hpp")[1]
            amend_file(f, m, ch_head, num-2, reset_contents)
            num = find_string(f, m, "BE_FUNCTION")[1]
            amend_file(f, m, "BE_ALLOW_MODELS({0})".format(gum.name), num-2,
                       reset_contents)
        # micrOMEGAs
        if output_opts.mo:
            print("Patching micrOMEGAs...")
            # Pristine and patched micrOMEGAs
            pristine_mo_dir = output_dir + "/micrOMEGAs"
            patched_mo_dir = output_dir + "/micrOMEGAs_patched"
            # Copy micromegas files to Backend patches from the cleaned
            # CalcHEP directory
            copy_micromegas_files(gum.name, reset_contents)
            # Add the patch file to the directory too
            patch_micromegas(gum.name, reset_contents)
            # Now write the headers
            ver = "3.6.9.2"
            be = "MicrOmegas_" + gum.name
            be_loc = "micromegas/{0}/{1}/libmicromegas.so".format(ver, gum.name)
            f = "frontends/MicrOmegas_{0}_{1}".format(gum.name,
                                                      ver.replace('.','_'))
            write_file(f+".cpp", m, mo_src, reset_contents)
            write_file(f+".hpp", m, mo_head, reset_contents)
            add_to_backend_locations(be, be_loc, ver, reset_contents)
            add_micromegas_to_cmake(gum.name, reset_contents)
            add_micromegas_to_darkbit_rollcall(gum.name, reset_contents,
                                               gum.dm_decays)

        # Pythia
        if output_opts.pythia:
            be = "pythia_"+gum.name.lower()
            safe_ver = "8_"+base_pythia_version
            ver = "8."+base_pythia_version
            print("Creating BOSS config files for Pythia_"+gum.name+".")
            write_boss_configs_for_pythia(gum.name, output_dir,reset_contents)
            copy_file(be+"/"+ver+"/patch_"+be+"_"+ver+".dif", m, output_dir,
                      reset_contents, existing = False)
            add_to_backend_locations("Pythia_"+gum.name,
                                     be+"/"+ver+"/lib/libpythia8.so", ver,
                                     reset_contents)
            add_to_backends_cmake(pythia_cmake, reset_contents,
                                  string_to_find="# Nulike")
            add_to_default_bossed_version("Pythia_"+gum.name,
                                          safe_ver, reset_contents)

        # SPheno
        if output_opts.spheno:
            ver = SPHENO_VERSION
            be = "SARAHSPheno_"+gum.name
            be_loc = (
                   "sarah-spheno/{0}/{1}/lib/libSPheno{2}.so"
            ).format(ver, gum.name, clean_model_name)
            f = "frontends/SARAHSPheno_{0}_{1}".format(gum.name,
                                                       ver.replace('.','_'))
            write_file(f+".cpp", m, spheno_src, reset_contents)
            write_file(f+".hpp", m, spheno_header, reset_contents)
            add_to_backend_locations(be, be_loc, ver, reset_contents)
            add_to_backends_cmake(spheno_cmake, reset_contents,
                                  string_to_find="# gm2calc")
            # Move SPheno files to Backend/patches/...
            copy_spheno_files_gambit(gum.name, clean_model_name, ver, output_dir, reset_contents)
            patchloc = (
                     "sarah-spheno/{0}/{1}/patch_sarah-spheno_{0}_{1}.dif"
            ).format(ver, gum.name)
            mkdir_if_absent(os.path.dirname(full_filename(patchloc,m)),reset_contents)
            copy_file(patchloc, m, output_dir, reset_contents, existing = False)
            # SPheno DecayTable
            filename = (
                "data/SARAHSPheno_{0}_{1}_decays_info.dat"
            ).format(gum.name, SPHENO_VERSION.replace('.','_'))
            write_file(filename, m, spheno_decay_tables, reset_contents)
            # SPheno backend_types
            amend_file("backend_types/SPheno.hpp", m, backend_types, btnum-2,
                       reset_contents)
            # Check if backend needs rebuild
            be_install_dir  = "../Backends/installed/sarah-spheno/" + ver + "/" + gum.name
            gum_patched_dir = output_dir + '/SPheno_patched'
            check_backend_rebuild('sarah-spheno_'+gum.name, ver, be_install_dir,
                                  gum_patched_dir, rebuild_backends,
                                  file_endings=('F90','f90','Makefile'),
                                  build_dir='../build')

        # Vevacious
        if output_opts.vev:
            copy_vevacious_files(gum.name, outputs.get_vev(), reset_contents)
            num = find_string("SpecBit_VS.cpp", "SpecBit",
                              "} // end namespace SpecBit")[1]
            amend_file("SpecBit_VS.cpp", "SpecBit", vev_src, num-1,
                       reset_contents)
            write_vevacious_rollcall(gum.name, gum.spec, reset_contents)

        # Write capability and model definitions
        write_capability_definitions("capabilities.dat", gum.name, capability_definitions, reset_contents)
        write_model_definitions("models.dat", gum.name, model_definitions, reset_contents)

        # Generate a file containing all of the bib tags for the backends used.
        bibtags = generate_bib_tags(output_opts,gum.math)
        num = find_string("citation_keys.hpp", "Utils","      // GUM additions")[1]
        amend_file("citation_keys.hpp", "Utils", bibtags, num,reset_contents)

        # Write a simple YAML file.
        drop_yaml_file(gum.name, model_parameters, add_higgs, reset_contents,
                       gum.spec, output_opts.spheno)
        print(("\nGUM has dropped a test YAML file at "
               "$GAMBIT/yaml_files/{0}_example.yaml!").format(gum.name))

        # Inform user to rebuild backends
        if rebuild_backends:
            print(("The following backends have been changed, if you are not using "
                   "the config script provided make sure to rebuild them: "
                   "{0}").format(rebuild_backends))

        # Save output to a .mug file for resetting purposes
        mug_file = "mug_files/{0}.mug".format(gum.name)
        drop_mug_file(mug_file, reset_contents)

        # Remove the temp file, don't need it now.
        os.remove("mug_files/temp.mug")

        print("")
        print("Changes saved to {}".format(mug_file))
        print("If you need to reset GAMBIT, do:")
        print("\t./gum -r {}".format(mug_file))

        # All done!
        print("")
        print("GUM has finished successfully!")
        # Prints the recommended build commands to stdout.
        write_config_file(output_opts, gum.name, reset_contents, rebuild_backends)

    except Exception as exc:
        print("\n\nGUM has failed!")
        if os.path.exists("mug_files/temp.mug"):
            print("Removing all added content...")
            blockPrint()                     # Stop the output for a sec
            revert("mug_files/temp.mug")     # Remove it all
            enablePrint()                    # Allow printing again
            os.remove("mug_files/temp.mug")  # Remove the temp mug file
            print("\nError message:\n")
        import traceback
        traceback.print_exc()


# If reset is called
elif args.reset:
    try:
        revert(args.reset)
        model = os.path.split(args.reset)[-1].strip('.mug')
        print("GUM has removed the model '{0}' from GAMBIT.".format(model))
    except Exception as exc:
        import traceback
        traceback.print_exc()

else:
    raise GumError(("\n\n\tHi! You must be new here (or you're just excited)!"
                    "\n\n\tUsage: gum -f inifile.gum"
                    "\n\n\tOr try gum -h for help.\n"))
